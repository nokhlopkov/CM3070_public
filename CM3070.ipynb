{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a8104db-00a5-404c-987b-d32126743709",
   "metadata": {},
   "source": [
    "# Vision Transformer for Malware Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1670ed-74da-485b-869e-c6f3109c4880",
   "metadata": {},
   "source": [
    "## Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f34878-1338-4933-9b00-cf1a57425790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa #For GPU training\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import backend as k\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "from tensorflow.compat.v1.profiler import profile, ProfileOptionBuilder\n",
    "\n",
    "from keras_tuner import HyperModel\n",
    "from keras_tuner import RandomSearch\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from scipy import interp\n",
    "\n",
    "from classification_models.tfkeras import Classifiers\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import os #for file reading and writing|\n",
    "import shutil\n",
    "import copy\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff6d73f-dca4-48a5-bc7a-73572c4f36c6",
   "metadata": {},
   "source": [
    "## Data preprocessing: Load the source data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c312410-0c86-40d3-a558-68c7e8025477",
   "metadata": {},
   "source": [
    "The following code is taken from https://github.com/safreita1/malnet-image/blob/master/process.py, and is a recommended way to assemble the dataset. It was modified to solve a few path building issues and to provide a way to construct a combined train-val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9031c3b6-b1c5-429f-8845-9d5a23e1e7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_types(args, lines):\n",
    "    \"\"\"\n",
    "    Filters out files of specific types ('adware', 'trojan', 'benign', 'riskware') from a list of files. \n",
    "    The function checks the type of each file and only includes it in the output if it's not one of the types to exclude.\n",
    "\n",
    "    Args:\n",
    "        args (dict): A dictionary containing the following key:\n",
    "            'image_dir': The directory where the image files are stored.\n",
    "        lines (list): A list of file names (strings) without the directory path and file extension.\n",
    "\n",
    "    Returns:\n",
    "        filtered_files (list): A list of file paths for the files that are not of the types to exclude. The file paths include \n",
    "        the directory path and file extension.\n",
    "\n",
    "    \"\"\"\n",
    "    types_to_exclude = ['adware', 'trojan', 'benign', 'riskware']\n",
    "\n",
    "    filtered_files = []\n",
    "    files = [args['image_dir'] + file.strip() + '.png' for file in lines]\n",
    "\n",
    "    for file in files:\n",
    "        mtype = file.split('malnet-images/')[1].rsplit('/', 2)[0]\n",
    "        if mtype not in types_to_exclude:\n",
    "            filtered_files.append(file)\n",
    "\n",
    "    return filtered_files\n",
    "\n",
    "def get_split_info(args):\n",
    "    \"\"\"\n",
    "    Retrieves the data file paths and labels for the training, validation, and test datasets. The datasets are \n",
    "    organized based on the group type ('binary', 'type', or 'family') and the validation split defined in the 'args' \n",
    "    dictionary. The function also prints the paths for the split information and the number of samples in each dataset.\n",
    "\n",
    "    Args:\n",
    "        args (dict): A dictionary containing the following keys:\n",
    "            'group': A string indicating the group type. This can be 'binary', 'type', or 'family'.\n",
    "            'validation_split': A string indicating the validation split.\n",
    "            'image_dir': The directory where the image files are stored.\n",
    "            'malnet_tiny': A boolean flag to determine whether to use a small dataset or not.\n",
    "\n",
    "    Returns:\n",
    "        files_train (list): A list of file paths for the training images.\n",
    "        files_val (list): A list of file paths for the validation images.\n",
    "        files_test (list): A list of file paths for the test images.\n",
    "        train_labels (list): A list of labels for the training images.\n",
    "        val_labels (list): A list of labels for the validation images.\n",
    "        test_labels (list): A list of labels for the test images.\n",
    "        label_dict (dict): A dictionary mapping the label names to their corresponding integer indices.\n",
    "\n",
    "    Raises:\n",
    "        SystemExit: If the 'group' type is not one of 'binary', 'type', or 'family'.\n",
    "    \"\"\"\n",
    "    # 30.06.2023 fixed the path builder string to include split varition paths\n",
    "    print('training split path: /split_info/{}/{}/train.txt'.format(args['group'], args['validation_split']))\n",
    "    with open(os.getcwd() + '/split_info/{}/{}/train.txt'.format(args['group'], args['validation_split']), 'r') as f:\n",
    "        lines_train = f.readlines()\n",
    "\n",
    "    print('validation split path: /split_info/{}/{}/val.txt'.format(args['group'], args['validation_split']))\n",
    "    with open(os.getcwd() + '/split_info/{}/{}/val.txt'.format(args['group'], args['validation_split']), 'r') as f:\n",
    "        lines_val = f.readlines()\n",
    "\n",
    "    print('test split path: /split_info/{}/{}/test.txt'.format(args['group'], args['validation_split']))\n",
    "    with open(os.getcwd() + '/split_info/{}/{}/test.txt'.format(args['group'], args['validation_split']), 'r') as f:\n",
    "        lines_test = f.readlines()\n",
    "\n",
    "    if args['malnet_tiny']:\n",
    "        files_train = filter_types(args, lines_train)\n",
    "        files_val = filter_types(args, lines_val)\n",
    "        files_test = filter_types(args, lines_test)\n",
    "    else:\n",
    "        files_train = [args['image_dir'] + file.strip() + '.png' for file in lines_train]\n",
    "        files_val = [args['image_dir'] + file.strip() + '.png' for file in lines_val]\n",
    "        files_test = [args['image_dir'] + file.strip() + '.png' for file in lines_test]\n",
    "\n",
    "    if args['group'] == 'type':\n",
    "        labels = sorted(list(set([file.split('malnet-images/')[1].rsplit('/', 2)[0] for file in files_train])))\n",
    "        label_dict = {t: idx for idx, t in enumerate(labels)}\n",
    "\n",
    "        train_labels = [label_dict[file.split('malnet-images/')[1].rsplit('/', 2)[0]] for file in files_train]\n",
    "        val_labels = [label_dict[file.split('malnet-images/')[1].rsplit('/', 2)[0]] for file in files_val]\n",
    "        test_labels = [label_dict[file.split('malnet-images/')[1].rsplit('/', 2)[0]] for file in files_test]\n",
    "\n",
    "    elif args['group'] == 'family':\n",
    "        labels = sorted(list(set([file.split('malnet-images/')[1].rsplit('/', 2)[1] for file in files_train])))\n",
    "        label_dict = {t: idx for idx, t in enumerate(labels)}\n",
    "\n",
    "        train_labels = [label_dict[file.split('malnet-images/')[1].rsplit('/', 2)[1]] for file in files_train]\n",
    "        val_labels = [label_dict[file.split('malnet-images/')[1].rsplit('/', 2)[1]] for file in files_val]\n",
    "        test_labels = [label_dict[file.split('malnet-images/')[1].rsplit('/', 2)[1]] for file in files_test]\n",
    "\n",
    "    elif args['group'] == 'binary':\n",
    "        labels = ['benign', 'malicious']\n",
    "        label_dict = {t: idx for idx, t in enumerate(labels)}\n",
    "\n",
    "        train_labels = [0 if 'benign' in file.split('malnet-images/')[1].rsplit('/', 2)[0] else 1 for file in files_train]\n",
    "        val_labels = [0 if 'benign' in file.split('malnet-images/')[1].rsplit('/', 2)[0] else 1 for file in files_val]\n",
    "        test_labels = [0 if 'benign' in file.split('malnet-images/')[1].rsplit('/', 2)[0] else 1 for file in files_test]\n",
    "\n",
    "    else:\n",
    "        print('Group does not exist')\n",
    "        exit(1)\n",
    "\n",
    "    print('Number of train samples: {}, val samples: {}, test samples: {}'.format(len(files_train), len(files_val), len(files_test)))\n",
    "\n",
    "    return files_train, files_val, files_test, train_labels, val_labels, test_labels, label_dict\n",
    "\n",
    "def create_image_symlinks(args):\n",
    "    \"\"\"\n",
    "    Creates symbolic links (symlinks) for the image files in the training, validation, combined train-val and test datasets. The symlinks are \n",
    "    organized based on the group type ('binary' or 'family') defined in the 'args' dictionary. The function first retrieves \n",
    "    the data file paths for the datasets and then creates symlinks for each file in the appropriate directory.\n",
    "\n",
    "    Args:\n",
    "        args (dict): A dictionary containing the following keys:\n",
    "            'group': A string indicating the group type. This can be 'binary' or 'family'.\n",
    "            'image_dir': The directory where the image files are stored.\n",
    "            'data_dir': The directory where the symlinks will be created.\n",
    "\n",
    "    \"\"\"\n",
    "    print('Creating image symlinks')\n",
    "\n",
    "    files_train, files_val, files_test, _, _, _, _ = get_split_info(args)\n",
    "\n",
    "    # create symlinks for train/val/test folders\n",
    "    dst_dir = args['data_dir'] + '{}/'.format(args['group'])\n",
    "\n",
    "    for src_path in files_train:\n",
    "        dst_path = src_path.replace(args['image_dir'], dst_dir + 'train/')\n",
    "\n",
    "        if args['group'] == 'binary':\n",
    "            if 'benign' not in dst_path:\n",
    "                dst_path = dst_path.split('train/')[0] + 'train/malicious/' + dst_path.split('train/')[1].split('/')[2]\n",
    "            else:\n",
    "                dst_path = dst_path.split('train/')[0] + 'train/benign/' + dst_path.split('train/')[1].split('/')[2]\n",
    "\n",
    "        elif args['group'] == 'family':\n",
    "            dst_path = dst_path.split('train/')[0] + 'train/' + '/'.join(dst_path.split('train/')[1].split('/')[1:3])\n",
    "\n",
    "        if not os.path.exists(dst_path):\n",
    "            os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "            os.symlink(os.getcwd() + src_path, dst_path) # 30.06.2023 fixed a bug that led to creation of broken symlinks \n",
    "\n",
    "    for src_path in files_val:\n",
    "        dst_path = src_path.replace(args['image_dir'], dst_dir + 'val/')\n",
    "\n",
    "        if args['group'] == 'binary':\n",
    "            if 'benign' not in dst_path:\n",
    "                dst_path = dst_path.split('val/')[0] + 'val/malicious/' + dst_path.split('val/')[1].split('/')[2]\n",
    "            else:\n",
    "                dst_path = dst_path.split('val/')[0] + 'val/benign/' + dst_path.split('val/')[1].split('/')[2]\n",
    "\n",
    "        elif args['group'] == 'family':\n",
    "            dst_path = dst_path.split('val/')[0] + 'val/' + '/'.join(dst_path.split('val/')[1].split('/')[1:3])\n",
    "\n",
    "        if not os.path.exists(dst_path):\n",
    "            os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "            os.symlink(os.getcwd() + src_path, dst_path)\n",
    "\n",
    "    for src_path in files_test:\n",
    "        dst_path = src_path.replace(args['image_dir'], dst_dir + 'test/')\n",
    "\n",
    "        if args['group'] == 'binary':\n",
    "            if 'benign' not in dst_path:\n",
    "                dst_path = dst_path.split('test/')[0] + 'test/malicious/' + dst_path.split('test/')[1].split('/')[2]\n",
    "            else:\n",
    "                dst_path = dst_path.split('test/')[0] + 'test/benign/' + dst_path.split('test/')[1].split('/')[2]\n",
    "\n",
    "        elif args['group'] == 'family':\n",
    "            dst_path = dst_path.split('test/')[0] + 'test/' + '/'.join(dst_path.split('test/')[1].split('/')[1:3])\n",
    "\n",
    "        if not os.path.exists(dst_path):\n",
    "            os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "            os.symlink(os.getcwd() + src_path, dst_path)\n",
    "\n",
    "    for file_list in [files_train, files_val]:\n",
    "        for src_path in file_list:\n",
    "            dst_path = src_path.replace(args['image_dir'], dst_dir + 'trainval/')\n",
    "\n",
    "            if args['group'] == 'binary':\n",
    "                if 'benign' not in dst_path:\n",
    "                    dst_path = dst_path.split('trainval/')[0] + 'trainval/malicious/' + dst_path.split('trainval/')[1].split('/')[2]\n",
    "                else:\n",
    "                    dst_path = dst_path.split('trainval/')[0] + 'trainval/benign/' + dst_path.split('trainval/')[1].split('/')[2]\n",
    "\n",
    "            elif args['group'] == 'family':\n",
    "                dst_path = dst_path.split('trainval/')[0] + 'trainval/' + '/'.join(dst_path.split('trainval/')[1].split('/')[1:3])\n",
    "\n",
    "            if not os.path.exists(dst_path):\n",
    "                os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "                os.symlink(os.getcwd() + src_path, dst_path)\n",
    "    \n",
    "    print('Finished creating symlinks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51e58ce-4802-4e63-b401-49e643eaae08",
   "metadata": {},
   "source": [
    "## Data preprocessing: Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8923f3a-dc70-491b-be7b-19145eca2469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This implements the Gauss noise as augmentation technique, as suggested in Catak et al's paper\n",
    "@tf.function\n",
    "def add_gaussian_noise(image):\n",
    "    with tf.device('/GPU:0'):\n",
    "        mean = 0.0\n",
    "        var = 0.4\n",
    "        sigma = tf.math.sqrt(var)\n",
    "        gauss = tf.random.normal(shape=tf.shape(image), mean=mean, stddev=sigma)\n",
    "        return image + gauss\n",
    "\n",
    "# Keras ImageGenerator wrapper class that produces input images on both X and Y side\n",
    "# Used for self-supervised training tasks\n",
    "class SSTImageGenerator():\n",
    "    def __init__(self, generator, directory, batch_size, img_height, img_width, class_mode, color_mode, seed, shuffle):\n",
    "        self.generator = generator.flow_from_directory(directory, target_size=(img_height, img_width),\n",
    "                                                       class_mode=class_mode, color_mode=color_mode,\n",
    "                                                       batch_size=batch_size, seed=seed, shuffle=shuffle)\n",
    "        self.current_index = 0\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.generator)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        original_batch = self.generator.__getitem__(index)\n",
    "        x = original_batch[0]\n",
    "        \n",
    "        return x, x  # return the same data as both input and target\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_index >= len(self):\n",
    "            self.current_index = 0\n",
    "            raise StopIteration\n",
    "        result = self.__getitem__(self.current_index)\n",
    "        self.current_index += 1\n",
    "        return result\n",
    "\n",
    "    @property\n",
    "    def batch_index(self):\n",
    "        return self.generator.batch_index\n",
    "        \n",
    "    @property\n",
    "    def samples(self):\n",
    "        return self.generator.samples\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return self.generator.num_classes\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_index = 0\n",
    "        self.generator.reset()\n",
    "\n",
    "# Visualize an unmasked/masked sample for debugging\n",
    "def visualize_masking_sample(data_gen, image_size, patch_size, num_channels, projection_dim, mask_rate):\n",
    "    # Get normal samples\n",
    "    batch, _ = data_gen.__getitem__(0)\n",
    "    data_gen.reset()\n",
    "    \n",
    "    print(image_size, patch_size, num_channels)\n",
    "    patch_layer = Patches(patch_size, num_channels)\n",
    "    sample_patches = patch_layer(images=batch)\n",
    "    patch_encoder = PatchEncoder((image_size // patch_size) ** 2, projection_dim, mask_rate)\n",
    "    reconstructor = RearrangePatchesToImage(patch_size, image_size, num_channels)\n",
    "    \n",
    "    masked_patches = patch_encoder.generate_masked_patches(sample_patches)\n",
    "\n",
    "    # Reshaping sample_patches[0] to be compatible with PatchEncoder reconstruct_single_image\n",
    "    reshaped_sample_patch = tf.reshape(sample_patches[0], [image_size // patch_size, \n",
    "                                                             image_size // patch_size, \n",
    "                                                             patch_size, \n",
    "                                                             patch_size, \n",
    "                                                             num_channels])\n",
    "\n",
    "    reshaped_masked_patch = tf.reshape(masked_patches[0], [image_size // patch_size, \n",
    "                                                             image_size // patch_size, \n",
    "                                                             patch_size, \n",
    "                                                             patch_size, \n",
    "                                                             num_channels])\n",
    "    \n",
    "    plt.imshow(reconstructor.reconstruct_single_image(reshaped_sample_patch))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.imshow(reconstructor.reconstruct_single_image(reshaped_masked_patch))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c8f18a-7ef1-46dc-b274-ab34331efe3c",
   "metadata": {},
   "source": [
    "## Data preprocessing: Create image generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1be2018-aa11-4859-ab81-80e262be2d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generators(model_args, data_args):\n",
    "    \"\"\"\n",
    "    Creates data generators for the training, validation, and test datasets using the ImageDataGenerator class from Keras. \n",
    "    The datasets are organized based on the group type defined in the 'args' dictionary. The function creates a generator \n",
    "    for each dataset, specifying the directory, target size, class mode, color mode, batch size, seed, and shuffle settings.\n",
    "\n",
    "    Args:\n",
    "        args (dict): A dictionary containing the following keys:\n",
    "            'group': A string indicating the group type.\n",
    "            'data_dir': The directory where the datasets are stored.\n",
    "            'color_mode': The color mode for the images. Can be 'grayscale', 'rgb', etc.\n",
    "            'batch_size': The batch size for the generators.\n",
    "            'seed': A seed for the random number generator to ensure reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        train_gen (ImageDataGenerator): The data generator for the training data.\n",
    "        val_gen (ImageDataGenerator): The data generator for the validation data.\n",
    "        test_gen (ImageDataGenerator): The data generator for the test data.\n",
    "        trainval_gen (ImageDataGenerator): The data generator for the combined training and validation data.\n",
    "\n",
    "    \"\"\"\n",
    "    # Base ImageDataGenerator for normalization\n",
    "    idg = ImageDataGenerator(rescale=1. / 255)\n",
    "    if (data_args['use_gauss_noise']):\n",
    "        train_idg = ImageDataGenerator(rescale=1. / 255, preprocessing_function=add_gaussian_noise)\n",
    "    else:\n",
    "        train_idg = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "        val_gen = idg.flow_from_directory(directory='{}{}/val'.format(data_args['data_dir'], data_args['group']),\n",
    "                                           target_size=(model_args['image_size'], model_args['image_size']),\n",
    "                                           class_mode='categorical',\n",
    "                                           color_mode=data_args['color_mode'],\n",
    "                                           batch_size=model_args['batch_size'],\n",
    "                                           seed=model_args['seed'],\n",
    "                                           shuffle=False)\n",
    "    \n",
    "        test_gen = idg.flow_from_directory(directory='{}{}/test'.format(data_args['data_dir'], data_args['group']),\n",
    "                                            target_size=(model_args['image_size'], model_args['image_size']),\n",
    "                                            class_mode='categorical',\n",
    "                                            color_mode=data_args['color_mode'],\n",
    "                                            batch_size=model_args['batch_size'],\n",
    "                                            seed=model_args['seed'],\n",
    "                                            shuffle=False)\n",
    "\n",
    "    if (data_args['self-supervised']):\n",
    "        train_gen = SSTImageGenerator(generator=idg, directory='{}{}/train'.format(data_args['data_dir'], data_args['group']), \n",
    "                                            img_height=model_args['image_size'], \n",
    "                                            img_width=model_args['image_size'],\n",
    "                                            class_mode='categorical',\n",
    "                                            color_mode=data_args['color_mode'],\n",
    "                                            batch_size=model_args['batch_size'],\n",
    "                                            seed=model_args['seed'],\n",
    "                                            shuffle=True)\n",
    "\n",
    "        trainval_gen = SSTImageGenerator(generator=idg, directory='{}{}/trainval'.format(data_args['data_dir'], data_args['group']), \n",
    "                                            img_height=model_args['image_size'], \n",
    "                                            img_width=model_args['image_size'],\n",
    "                                            class_mode='categorical',\n",
    "                                            color_mode=data_args['color_mode'],\n",
    "                                            batch_size=model_args['batch_size'],\n",
    "                                            seed=model_args['seed'],\n",
    "                                            shuffle=True)\n",
    "        \n",
    "        test_gen = SSTImageGenerator(generator=idg, directory='{}{}/test'.format(data_args['data_dir'], data_args['group']),\n",
    "                                            img_height=model_args['image_size'], \n",
    "                                            img_width=model_args['image_size'],\n",
    "                                            class_mode='categorical',\n",
    "                                            color_mode=data_args['color_mode'],\n",
    "                                            batch_size=model_args['batch_size'],\n",
    "                                            seed=model_args['seed'],\n",
    "                                            shuffle=False)\n",
    "    else:\n",
    "        train_gen = train_idg.flow_from_directory(directory='{}{}/train'.format(data_args['data_dir'], data_args['group']),\n",
    "                                                           target_size=(model_args['image_size'], model_args['image_size']),\n",
    "                                                           class_mode='categorical',\n",
    "                                                           color_mode=data_args['color_mode'],\n",
    "                                                           batch_size=model_args['batch_size'],\n",
    "                                                           seed=model_args['seed'], shuffle=True)\n",
    "        trainval_gen = train_idg.flow_from_directory(directory='{}{}/trainval'.format(data_args['data_dir'], data_args['group']),\n",
    "                                                           target_size=(model_args['image_size'], model_args['image_size']),\n",
    "                                                           class_mode='categorical',\n",
    "                                                           color_mode=data_args['color_mode'],\n",
    "                                                           batch_size=model_args['batch_size'],\n",
    "                                                           seed=model_args['seed'], shuffle=True)\n",
    "\n",
    "    return train_gen, val_gen, trainval_gen, test_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aba1ce1-0693-4daf-af49-564c8e522c06",
   "metadata": {},
   "source": [
    "## Models components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aa577a-b5f3-41af-ac5b-52ebff628949",
   "metadata": {},
   "source": [
    "## Model components: Multi-layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7897170-7f98-494f-87a6-116ebcea6e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://keras.io/examples/vision/image_classification_with_vision_transformer/\n",
    "# Re-implemented as class, because calling other layers from Transformer block is only allowed with dynamic execution\n",
    "\n",
    "class MLP(keras.layers.Layer):\n",
    "    def __init__(self, hidden_units, dropout_rate):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.dense_layers = [keras.layers.Dense(unit, activation=tf.nn.gelu) for unit in hidden_units]\n",
    "        self.dropout_layers = [keras.layers.Dropout(dropout_rate) for _ in hidden_units]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for dense, dropout in zip(self.dense_layers, self.dropout_layers):\n",
    "            x = dense(x)\n",
    "            x = dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbf61ae-bf9e-46d4-9d23-c5ca2cd0c1ee",
   "metadata": {},
   "source": [
    "## Model components: Patches layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dacb434-72c2-480e-8a05-e2549efaeeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://keras.io/examples/vision/image_classification_with_vision_transformer/\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size, num_channels):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = self.patch_size * self.patch_size * self.num_channels\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])  # Use -1 for the first dimension\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14bc747-5b1c-471a-ba4a-a13fd9d2c871",
   "metadata": {},
   "source": [
    "## Model components: Patch encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e4d55f-f728-4a84-8724-7dd807b486a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also from https://keras.io/examples/vision/image_classification_with_vision_transformer/\n",
    "# Added patch masking for ViT-VS and ViT-SHERLOCK\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom layer to encode patches in a Vision Transformer.\n",
    "\n",
    "    Args:\n",
    "        num_patches: The number of patches.\n",
    "        projection_dim: The dimensionality of the projected patch embeddings.\n",
    "        mask_rate: Fraction of patches to be masked (set to zero). Value should be between 0 and 1.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_patches, projection_dim, mask_rate):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = keras.layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "        self.mask_rate = mask_rate\n",
    "\n",
    "    def generate_masked_patches(self, patches):\n",
    "        # Generate a binary mask\n",
    "        batch_size = tf.shape(patches)[0]\n",
    "        mask = tf.random.uniform((batch_size, self.num_patches)) >= self.mask_rate\n",
    "        mask = tf.cast(mask, dtype=patches.dtype)\n",
    "\n",
    "        # Apply the mask to the patches\n",
    "        masked_patches = patches * tf.reshape(mask, [-1, self.num_patches, 1])\n",
    "        \n",
    "        return masked_patches\n",
    "       \n",
    "    def call(self, patches):\n",
    "        masked_patches = self.generate_masked_patches(patches)\n",
    "        \n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(masked_patches) + self.position_embedding(positions)\n",
    "        \n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56750815-e086-4337-b49c-dfa0b380b773",
   "metadata": {},
   "source": [
    "## Model components: Tranformer block and Masked auto encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020b51d8-212f-45f0-b046-1d62ea3c61ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, projection_dim, num_heads):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.projection_dim = projection_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.transformer_units = [projection_dim * 2, projection_dim]\n",
    "\n",
    "        self.layer_norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.mha = layers.MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim, dropout=0.1)\n",
    "        self.layer_norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.mlp_block = MLP(self.transformer_units, dropout_rate=0.1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x1 = self.layer_norm1(inputs)\n",
    "        attention_output = self.mha(x1, x1)\n",
    "        x2 = layers.Add()([attention_output, inputs])\n",
    "\n",
    "        x3 = self.layer_norm2(x2)\n",
    "        mlp_output = self.mlp_block(x3)\n",
    "        output = layers.Add()([mlp_output, x2])\n",
    "\n",
    "        return output\n",
    "        \n",
    "class MaskedAutoEncoder(keras.Model):\n",
    "    def __init__(self, patch_size, projection_dim, num_heads, num_decoder_blocks=4):\n",
    "        super(MaskedAutoEncoder, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.decoder_blocks = [TransformerBlock(projection_dim, num_heads) for _ in range(num_decoder_blocks)]\n",
    "        self.linear_projection = layers.Dense(patch_size * patch_size * 3)\n",
    "        self.sigmoid_layer = keras.layers.Activation('sigmoid')\n",
    "\n",
    "    def call(self, hidden_states):\n",
    "        # Ducttaping the shaving of classficaiton token\n",
    "        if (current_experiment == 'ViT-SHERLOCK'):\n",
    "            x = hidden_states[:, 1:, :]\n",
    "        else:\n",
    "            x = hidden_states\n",
    "\n",
    "        for decoder_block in self.decoder_blocks:\n",
    "            x = decoder_block(x)\n",
    "\n",
    "        x = self.linear_projection(x)\n",
    "        reconstructed_patches = self.sigmoid_layer(x)\n",
    "\n",
    "        return reconstructed_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f156775-78ec-477f-9ca5-fb9feb7a4584",
   "metadata": {},
   "source": [
    "## Model components: Image reconstructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0c8526-9e67-4501-88dc-d2b4f96427cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ViT-VS and ViT-SHERLOCK, rearranges patches into images for MSELoss to calculate the error\n",
    "class RearrangePatchesToImage(layers.Layer):\n",
    "    def __init__(self, patch_size, image_size, num_channels):\n",
    "        super(RearrangePatchesToImage, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.image_size = image_size\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def call(self, patches):\n",
    "        # Reshape patches for assembly\n",
    "        patches_reshaped = tf.reshape(patches, (-1, self.image_size // self.patch_size, self.image_size // self.patch_size, self.patch_size, self.patch_size, self.num_channels))\n",
    "\n",
    "        # Use tf.map_fn to reconstruct each image\n",
    "        reconstructed_images = tf.map_fn(self.reconstruct_single_image, patches_reshaped, dtype=tf.float32)\n",
    "\n",
    "        return reconstructed_images\n",
    "\n",
    "    # Adaptation from https://keras.io/examples/vision/masked_image_modeling/\n",
    "    def reconstruct_single_image(self, single_image_patches):\n",
    "        # Use tf.split and tf.concat to assemble the patches into images\n",
    "        rows = [tf.concat(tf.unstack(row_patches, axis=0), axis=1) for row_patches in tf.unstack(single_image_patches, axis=0)]\n",
    "        reconstructed_image = tf.concat(rows, axis=0)\n",
    "        \n",
    "        return reconstructed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6208c626-6106-4f80-be1c-afd8bc23bb65",
   "metadata": {},
   "source": [
    "## Vision Transfomer\n",
    "The base code for builder functions is adapted from Keras ViT tutorial: https://keras.io/examples/vision/image_classification_with_vision_transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7854c753-6ce0-499c-846d-ca3950b2d358",
   "metadata": {},
   "source": [
    "## ViT-V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c18de33-f99d-4502-9e75-c939dc238082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ViT_V(data_gen, cfg, data_cfg):\n",
    "    inputs = layers.Input(shape=(cfg['image_size'], cfg['image_size'], cfg['num_channels']))\n",
    "\n",
    "    # Create patches\n",
    "    patches = Patches(cfg['patch_size'], cfg['num_channels'])(inputs)\n",
    "    \n",
    "    # Encode patches\n",
    "    encoded_patches = PatchEncoder((cfg['image_size'] // cfg['patch_size']) ** 2, cfg['projection_dim'], 0)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block\n",
    "    for _ in range(cfg['transformer_layers']):\n",
    "        transformer_block = TransformerBlock(cfg['projection_dim'], cfg['num_heads'])\n",
    "        encoded_patches = transformer_block(encoded_patches)\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    \n",
    "    # Add the MLP\n",
    "    mlp_block = MLP(hidden_units=cfg['mlp_head_units'], dropout_rate=0.5)\n",
    "    features = mlp_block(representation)\n",
    "\n",
    "    # Classify outputs\n",
    "    logits = keras.layers.Dense(data_gen.num_classes)(features)\n",
    "\n",
    "    # Create the Keras model\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2a2542-fee0-4148-9d0d-5c16f6483433",
   "metadata": {},
   "source": [
    "## ViT-B/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9962bab-9811-4dc1-903a-c2156a45ff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct ViT-B/16 from published weights\n",
    "# Weights are acquired from here:\n",
    "# https://github.com/sayakpaul/probing-vits/releases/download/v1.0.0/probing_vits.zip\n",
    "\n",
    "def create_ViT_B_16(data_gen, cfg, data_cfg):\n",
    "    # Define the custom input layer\n",
    "    inputs = keras.Input(shape=(cfg['image_size'], cfg['image_size'], cfg['num_channels']))\n",
    "    \n",
    "    # Load the pretrained model\n",
    "    pretrained_model = keras.models.load_model('models/ViT-B_16/vit_b16_patch16_224-i1k_pretrained', compile=False)\n",
    "\n",
    "    # Pass the custom input through the pretrained model\n",
    "    x = pretrained_model(inputs)\n",
    "\n",
    "    # Add the custom output layer for your classification task\n",
    "    logits = keras.layers.Dense(data_gen.num_classes)(x)\n",
    "\n",
    "    # Create a new model with the custom input and output\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794514a6-abba-4bd6-a8e3-5a1b00b2823b",
   "metadata": {},
   "source": [
    "## ViT-SHERLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96907337-d098-422d-854e-adf4f1e4368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ViT_SHERLOCK(data_gen, cfg, data_cfg):\n",
    "    # Define the custom input layer\n",
    "    inputs = keras.Input(shape=(cfg['image_size'], cfg['image_size'], cfg['num_channels']))\n",
    "    patches_layer = Patches(cfg['patch_size'], cfg['num_channels'])(inputs)\n",
    "    encoded_patches = PatchEncoder((cfg['image_size'] // cfg['patch_size']) ** 2, cfg['encoder_projection_dim'], cfg['mask_rate'])(patches_layer)\n",
    "\n",
    "    # Classification token, otherwise ViT-B/16 base doesn't work\n",
    "    batch_size = tf.shape(encoded_patches)[0]\n",
    "    cls_token = tf.keras.layers.Embedding(input_dim=1, output_dim=cfg['encoder_projection_dim'])(tf.zeros((batch_size, 1), dtype=tf.int32))\n",
    "    encoded_patches = layers.Concatenate(axis=1)([cls_token, encoded_patches])\n",
    "            \n",
    "    if (data_cfg['self-supervised']):\n",
    "        # Load ViT-B/16 weights\n",
    "        pretrained_model = keras.models.load_model('models/ViT-B_16/vit_b16_patch16_224-i1k_pretrained', compile=False)\n",
    "        \n",
    "        encoder_layers = pretrained_model.layers[1:-3]\n",
    "    \n",
    "        for layer in encoder_layers:\n",
    "            if 'transformer_block' in layer.name:\n",
    "                encoded_patches = layer(encoded_patches)[0] \n",
    "                encoded_patches = layers.Lambda(lambda x: x, name=f\"lambda_{layer.name}\")(encoded_patches)\n",
    "            else:\n",
    "                encoded_patches = layer(encoded_patches)\n",
    "                \n",
    "        # Project down the encoded patches before decoding\n",
    "        projection_layer = layers.Dense(cfg['decoder_projection_dim'], activation='linear')\n",
    "        projected_encoded_patches = projection_layer(encoded_patches)\n",
    "        \n",
    "        # Decode the projected encoded patches using the MaskedAutoEncoder\n",
    "        mae = MaskedAutoEncoder(cfg['patch_size'], cfg['decoder_projection_dim'], cfg['num_heads'])\n",
    "        reconstructed_patches = mae(projected_encoded_patches)\n",
    "    \n",
    "        # Reconstruct the images from patches\n",
    "        reconstructed_images = RearrangePatchesToImage(cfg['patch_size'], cfg['image_size'], cfg['num_channels'])(reconstructed_patches)\n",
    "    \n",
    "        model = keras.Model(inputs=inputs, outputs=reconstructed_images)\n",
    "    else:\n",
    "        pretrained_model = keras.models.load_model('models/ViT-B_16/vit_b16_patch16_224-i1k_pretrained', compile=False)\n",
    "        # x = pretrained_model(inputs)\n",
    "        encoder_layers = pretrained_model.layers[1:-3]\n",
    "        pretrained_model.summary()\n",
    "        \n",
    "        for layer in encoder_layers:\n",
    "            if 'transformer_block' in layer.name:\n",
    "                encoded_patches = layer(encoded_patches)[0] \n",
    "                encoded_patches = layers.Lambda(lambda x: x, name=f\"lambda_{layer.name}\")(encoded_patches)\n",
    "            else:\n",
    "                encoded_patches = layer(encoded_patches)\n",
    "       \n",
    "        # Create a [batch_size, projection_dim] tensor\n",
    "        representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        representation = layers.Flatten()(representation)\n",
    "        representation = layers.Dropout(0.5)(representation)\n",
    "\n",
    "        # Classify outputs\n",
    "        logits = keras.layers.Dense(data_gen.num_classes)(representation)\n",
    "    \n",
    "        # Create the Keras model\n",
    "        model = keras.Model(inputs=inputs, outputs=logits)       \n",
    "        model.load_weights('models/ViT-SHERLOCK/encodings/ViT-SHERLOCK_encodings.h5', by_name=True)\n",
    "        model.summary()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63521ac9-2e6a-40de-a7c2-e6f4aa701217",
   "metadata": {},
   "source": [
    "## ViT-VS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ae96c0-c02b-4dfe-8cf7-ff39c90e0462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The encoder part is only trainable during self-supervised training stage\n",
    "# Only the classifier head is tranable for classification\n",
    "\n",
    "def create_ViT_VS(data_gen, cfg, data_cfg):\n",
    "    inputs = layers.Input(shape=(cfg['image_size'], cfg['image_size'], cfg['num_channels']))\n",
    "    \n",
    "    patches_layer = Patches(cfg['patch_size'], cfg['num_channels'])(inputs)\n",
    "    \n",
    "    # Encode patches\n",
    "    encoded_patches = PatchEncoder((cfg['image_size'] // cfg['patch_size']) ** 2, cfg['encoder_projection_dim'], cfg['mask_rate'])(patches_layer)\n",
    "\n",
    "    # Apply transformer blocks for encoding\n",
    "    for _ in range(cfg['encoder_layers']):\n",
    "        transformer_block = TransformerBlock(cfg['encoder_projection_dim'], cfg['num_heads'])\n",
    "        encoded_patches = transformer_block(encoded_patches)\n",
    "\n",
    "    if (data_cfg['self-supervised']):\n",
    "        # Project down the encoded patches before decoding\n",
    "        projection_layer = layers.Dense(cfg['decoder_projection_dim'], activation='linear')\n",
    "        projected_encoded_patches = projection_layer(encoded_patches)\n",
    "        \n",
    "        # Decode the projected encoded patches using the MaskedAutoEncoder\n",
    "        mae = MaskedAutoEncoder(cfg['patch_size'], cfg['decoder_projection_dim'], cfg['num_heads'])\n",
    "        reconstructed_patches = mae(projected_encoded_patches)\n",
    "    \n",
    "        # Reconstruct the images from patches\n",
    "        reconstructed_images = RearrangePatchesToImage(cfg['patch_size'], cfg['image_size'], cfg['num_channels'])(reconstructed_patches)\n",
    "    \n",
    "        model = keras.Model(inputs=inputs, outputs=reconstructed_images)\n",
    "    else:\n",
    "        # Classifier head - identical to ViT-V\n",
    "        # Create a [batch_size, projection_dim] tensor\n",
    "        representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        representation = layers.Flatten()(representation)\n",
    "        representation = layers.Dropout(0.5)(representation)\n",
    "        \n",
    "        # Add the MLP\n",
    "        mlp_block = MLP(hidden_units=cfg['mlp_head_units'], dropout_rate=0.5)\n",
    "        features = mlp_block(representation)\n",
    "    \n",
    "        # Classify outputs\n",
    "        logits = keras.layers.Dense(data_gen.num_classes)(representation)\n",
    "    \n",
    "        # Create the Keras model\n",
    "        model = keras.Model(inputs=inputs, outputs=logits)        \n",
    "        model.load_weights('models/ViT-VS/encodings/ViT-VS_encodings.h5', by_name=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1613b29-845c-4410-af6e-e25df4e0a678",
   "metadata": {},
   "source": [
    "## Pre-built CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c035065d-66f8-46fa-a825-c22165ed8ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_MobileNetV2(data_gen, params):\n",
    "    base_model = MobileNetV2(input_shape=(params['image_size'], params['image_size'], params['num_channels']), \n",
    "                             weights=None, include_top=False)\n",
    "    \n",
    "    x = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    output = keras.layers.Dense(data_gen.num_classes, activation='softmax')(x)\n",
    "    model = keras.models.Model(inputs=base_model.input, outputs=output)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "def create_ResNet18(data_gen, params):\n",
    "    ResNet18, _ = Classifiers.get('resnet18')\n",
    "    base_model = ResNet18((params['image_size'], params['image_size'], params['num_channels']), \n",
    "                          weights=None, include_top=False)\n",
    "    \n",
    "    x = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    output = keras.layers.Dense(data_gen.num_classes, activation='softmax')(x)\n",
    "    model = keras.models.Model(inputs=base_model.input, outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c75ddb2-6011-4082-8f43-b133091a8357",
   "metadata": {},
   "source": [
    "## Model building and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4d9152-f853-495b-adfd-92b9eb9c7b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builder function mapping\n",
    "model_builders = {\n",
    "    'ViT-V': create_ViT_V,\n",
    "    'ViT-VS': create_ViT_VS,\n",
    "    'ViT-SHERLOCK': create_ViT_SHERLOCK, #ViT-SHERLOCK is ViT-VS, but with different parameters #, create_ViT_SHERLOCK, \n",
    "    'ViT-B_16': create_ViT_B_16,\n",
    "    'ResNet18': create_ResNet18,\n",
    "    'MobileNetV2': create_MobileNetV2\n",
    "}\n",
    "\n",
    "# Implementation of class weighting by effective number of samples used in Freitas et al\n",
    "def get_class_weights(data_gen, reweigh, beta=0.999):\n",
    "    num_classes = data_gen.num_classes\n",
    "    labels = []\n",
    "\n",
    "    # Continue iterating until all classes have been seen\n",
    "    while len(np.unique(labels)) < num_classes:\n",
    "        for _, batch_labels in data_gen:\n",
    "            labels += list(np.argmax(batch_labels, axis=-1))\n",
    "            if len(np.unique(labels)) >= num_classes:\n",
    "                break\n",
    "\n",
    "    labels = np.array(labels)\n",
    "    counter = Counter(labels)\n",
    "    total_samples = len(labels)\n",
    "\n",
    "    if reweigh:\n",
    "        # Calculate effective numbers\n",
    "        effective_numbers = {cls: (1.0 - beta) / (1.0 - beta**count) for cls, count in counter.items()}\n",
    "        # Normalize the effective numbers\n",
    "        total_effective_number = sum(effective_numbers.values())\n",
    "        class_weight_dict = {cls: eff_num / total_effective_number for cls, eff_num in effective_numbers.items()}\n",
    "    else:\n",
    "        class_weight_dict = {cls: count / total_samples for cls, count in counter.items()}\n",
    "\n",
    "    data_gen.reset()\n",
    "\n",
    "    return class_weight_dict\n",
    "\n",
    "def build_model_for_validation(hp, model_name, train_gen, data_cfg):\n",
    "    model = model_builders[model_name](train_gen, experiments[model_name])\n",
    "    \n",
    "    output_logits = True\n",
    "    \n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "            learning_rate=hp.Choice('learning_rate', values=[experiments[model_name]['learning_rate']]),\n",
    "            weight_decay=hp.Choice('weight_decay', values=[experiments[model_name]['weight_decay']])\n",
    "    )\n",
    "\n",
    "    if (data_cfg['self-supervised']):\n",
    "        #loss=keras.losses.MeanSquaredError()\n",
    "        loss=MSELoss(experiments[model_name]['patch_size'])\n",
    "        #metrics=[keras.metrics.MeanSquaredError(name=\"mse\")]\n",
    "        metrics=[MSEMetric(experiments[model_name]['patch_size'], name=\"custom_mse\")]\n",
    "    else:\n",
    "        loss=keras.losses.CategoricalCrossentropy(from_logits=output_logits)\n",
    "        metrics=[keras.metrics.CategoricalAccuracy(name=\"accuracy\"), \n",
    "                 keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\")]\n",
    "        \n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_model_for_training(hp, model_name, train_gen, data_cfg):\n",
    "    model = model_builders[model_name](train_gen, experiments[model_name], data_cfg)\n",
    "    \n",
    "    output_logits = True\n",
    "\n",
    "    if model_name == \"ResNet18\":\n",
    "        output_logits = False\n",
    "        #optimizer = tfa.optimizers.SGDW(learning_rate=hp['learning_rate'], momentum=0.9, weight_decay=hp['weight_decay'])\n",
    "        optimizer = tfa.optimizers.AdamW(learning_rate=hp['learning_rate'], weight_decay=hp['weight_decay'])\n",
    "    elif model_name == \"MobileNetV2\":\n",
    "        output_logits = False\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=hp['learning_rate'],\n",
    "            decay_steps=1000,\n",
    "            decay_rate=0.98\n",
    "        )\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate=lr_schedule)\n",
    "    else:\n",
    "        optimizer = tfa.optimizers.AdamW(learning_rate=hp['learning_rate'], weight_decay=hp['weight_decay'])\n",
    "\n",
    "    if (data_cfg['self-supervised']):\n",
    "        #loss=keras.losses.MeanSquaredError()\n",
    "        loss=MSELoss(experiments[model_name]['patch_size'])\n",
    "        #metrics=[keras.metrics.MeanSquaredError(name=\"mse\")]\n",
    "        metrics=[MSEMetric(experiments[model_name]['patch_size'], name=\"custom_mse\")]\n",
    "    else:\n",
    "        loss=keras.losses.CategoricalCrossentropy(from_logits=output_logits)\n",
    "        metrics=[keras.metrics.CategoricalAccuracy(name=\"accuracy\"), \n",
    "                 keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\")]\n",
    "        \n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    return model\n",
    "\n",
    "def validate_parameters(model, train_gen, val_gen, class_weights, model_directory, data_cfg):\n",
    "    if (data_cfg['self-supervised']):\n",
    "        objective='val_mse'\n",
    "    else:\n",
    "        objective='val_accuracy'\n",
    "   \n",
    "    tuner = RandomSearch(\n",
    "        hypermodel=lambda hp: build_model_for_validation(hp, model, train_gen),\n",
    "        objective=objective,\n",
    "        max_trials=1,\n",
    "        executions_per_trial=1,\n",
    "        directory=model_directory,\n",
    "        project_name=model,\n",
    "    )\n",
    "        \n",
    "    tuner.search_space_summary()\n",
    "\n",
    "    # Stop training if loss doesnt improve after 5 epochs\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=0)]\n",
    "\n",
    "    if (class_weights == None):\n",
    "        tuner.search(train_gen,\n",
    "                     validation_data=val_gen,\n",
    "                     steps_per_epoch=int(train_gen.samples / experiments[model]['batch_size']),\n",
    "                     validation_steps=int(val_gen.samples / experiments[model]['batch_size']),\n",
    "                     epochs=experiments[model]['validation_epochs'], callbacks=callbacks\n",
    "                     )\n",
    "    else:\n",
    "        tuner.search(train_gen,\n",
    "                     validation_data=val_gen,\n",
    "                     steps_per_epoch=int(train_gen.samples / experiments[model]['batch_size']),\n",
    "                     validation_steps=int(val_gen.samples / experiments[model]['batch_size']),\n",
    "                     epochs=experiments[model]['validation_epochs'], callbacks=callbacks, class_weight=class_weights\n",
    "                     )\n",
    "\n",
    "    return tuner\n",
    "\n",
    "# Custom callback to reset the generator at an end of training epoch\n",
    "class ManualGeneratorResetOEE(keras.callbacks.Callback):\n",
    "    def __init__(self, generator):\n",
    "        super(ManualGeneratorResetOEE, self).__init__()\n",
    "        self.generator = generator\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.generator.reset()\n",
    "\n",
    "# Custom loss function as per description in Seneviratne et al\n",
    "class MSEMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, patch_size, name=\"custom_mse\", **kwargs):\n",
    "        super(MSEMetric, self).__init__(name=name, **kwargs)\n",
    "        self.patch_size = patch_size\n",
    "        self.mse_sum = self.add_weight(name=\"mse_sum\", initializer=\"zeros\")\n",
    "        self.sample_count = self.add_weight(name=\"sample_count\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):       \n",
    "        y_pred_mean, y_pred_var = tf.nn.moments(y_pred, axes=[-1], keepdims=True)\n",
    "\n",
    "        # Resize to match the original dimensions\n",
    "        y_pred_mean_resized = tf.image.resize(y_pred_mean, [224, 224])\n",
    "        y_pred_std_resized = tf.sqrt(tf.image.resize(y_pred_var, [224, 224]) + 1e-7)\n",
    "\n",
    "        y_true_mean, y_true_var = tf.nn.moments(y_true, axes=[-1], keepdims=True)\n",
    "        y_true_std = tf.sqrt(y_true_var + 1e-7)\n",
    "\n",
    "        y_true_normalized = (y_true - y_true_mean) / y_true_std\n",
    "        y_pred_normalized = (y_pred - y_pred_mean_resized) / y_pred_std_resized\n",
    "\n",
    "        mse = tf.reduce_mean(tf.square(y_true_normalized - y_pred_normalized))\n",
    "        \n",
    "        # Update sum of MSE and sample count\n",
    "        self.mse_sum.assign_add(mse)\n",
    "        self.sample_count.assign_add(1)\n",
    "\n",
    "    def result(self):\n",
    "        return self.mse_sum / self.sample_count\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.mse_sum.assign(0.0)\n",
    "        self.sample_count.assign(0.0)\n",
    "        \n",
    "def MSELoss(patch_size):\n",
    "    def mean_error_over_pixes(y_true, y_pred):         \n",
    "        y_pred_mean, y_pred_var = tf.nn.moments(y_pred, axes=[-1], keepdims=True)\n",
    "\n",
    "        # Resize to match the original dimensions\n",
    "        y_pred_mean_resized = tf.image.resize(y_pred_mean, [224, 224])\n",
    "        y_pred_std_resized = tf.sqrt(tf.image.resize(y_pred_var, [224, 224]) + 1e-7)\n",
    "\n",
    "        y_true_mean, y_true_var = tf.nn.moments(y_true, axes=[-1], keepdims=True)\n",
    "        y_true_std = tf.sqrt(y_true_var + 1e-7)\n",
    "\n",
    "        y_true_normalized = (y_true - y_true_mean) / y_true_std\n",
    "        y_pred_normalized = (y_pred - y_pred_mean_resized) / y_pred_std_resized\n",
    "\n",
    "        mse = tf.reduce_mean(tf.square(y_true_normalized - y_pred_normalized))\n",
    "\n",
    "        return mse\n",
    "        \n",
    "    return mean_error_over_pixes\n",
    "    \n",
    "def train_model(model_name, train_gen, val_gen, trainval_gen, class_weights, data_cfg, model_directory):   \n",
    "    if (data_cfg['skip_validation']):\n",
    "        best_hps = {'learning_rate': experiments[model_name]['learning_rate'], 'weight_decay': experiments[model_name]['weight_decay']}\n",
    "        best_model = build_model_for_training(best_hps, model_name, train_gen, data_cfg)\n",
    "    else:\n",
    "        print(\"Parameter validation stage\")\n",
    "        tuner = validate_parameters(model_name, train_gen, val_gen, class_weights, model_directory, data_cfg)\n",
    "        best_hps = tuner.get_best_hyperparameters()[0].values\n",
    "        best_model = build_model_for_training(best_hps, model_name, train_gen, data_cfg)\n",
    "\n",
    "    with open(f'{model_directory}/{model_name}_summary.txt', 'w') as f:\n",
    "        best_model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "    if (data_cfg['self-supervised']):\n",
    "        monitor='custom_mse'\n",
    "    else:\n",
    "        monitor='val_loss'\n",
    "        \n",
    "    # If loss does not improve after 10 epochs, stop\n",
    "    es = keras.callbacks.EarlyStopping(monitor=monitor, patience=10, verbose=0)\n",
    "    mc = keras.callbacks.ModelCheckpoint(f'{model_directory}/checkpoints/{model_name}_{data_cfg[\"group\"]}.h5', monitor=monitor,save_best_only=True,save_weights_only=True)\n",
    "    gr = ManualGeneratorResetOEE(trainval_gen)\n",
    "        \n",
    "    rlrop = keras.callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=3, min_lr=1e-8)\n",
    "\n",
    "    if (model_name == 'ViT-VS' or model_name == 'ViT-SHERLOCK'):\n",
    "        visualize_masking_sample(trainval_gen, experiments[model_name]['image_size'], \n",
    "                                       experiments[model_name]['patch_size'], \n",
    "                                       experiments[model_name]['num_channels'],\n",
    "                                       experiments[model_name]['encoder_projection_dim'],\n",
    "                                       experiments[model_name]['mask_rate'] )\n",
    "        \n",
    "    if (data_cfg['self-supervised']): # ViT-VS and ViT-SHERLOCK       \n",
    "        history = best_model.fit(trainval_gen,\n",
    "                                 epochs=experiments[model_name]['num_epochs'], \n",
    "                                 steps_per_epoch=(trainval_gen.samples // experiments[model_name]['batch_size']), \n",
    "                                 callbacks=[es,mc,gr,rlrop]) # Enable LR reduction due to longer training time\n",
    "    else:\n",
    "        history = best_model.fit(trainval_gen,\n",
    "                                 validation_data=val_gen, \n",
    "                                 epochs=experiments[model_name]['num_epochs'], \n",
    "                                 steps_per_epoch=(train_gen.samples // experiments[model_name]['batch_size']), \n",
    "                                 callbacks=[es, mc],#, rlrop],\n",
    "                                 class_weight=class_weights)\n",
    "\n",
    "    return best_model, best_hps, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d9ad40-7a7f-4714-a61e-a439eacb75b6",
   "metadata": {},
   "source": [
    "## Model evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd40024a-90e2-4f8c-ae0b-842ed10a8002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, model_name, test_gen):\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(test_gen, steps=int(test_gen.samples / experiments[model_name]['batch_size']))\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Top-5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "    \n",
    "    test_gen.reset()  # To ensure the generator starts from the beginning\n",
    "\n",
    "    start_time = time.time()  # start timing\n",
    "    predictions = model.predict(test_gen, steps=np.ceil(test_gen.samples / experiments[model_name]['batch_size']))\n",
    "    end_time = time.time()  # end timing\n",
    "\n",
    "    inference_time = end_time - start_time\n",
    "\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return predicted_classes, inference_time\n",
    "\n",
    "def plot_trainval_graph(model_name, history, data_cfg, model_directory):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    if (data_cfg['self-supervised']):\n",
    "        plt.plot(history.history['custom_mse'])\n",
    "    else:\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        \n",
    "    plt.title(f'{model_name} loss ({data_cfg[\"group\"]} classification task)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.savefig(f'{model_directory}/{model_name}_{data_cfg[\"group\"]}_loss.png', dpi=800)\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_graph(model, model_name, test_gen, data_cfg, model_directory):\n",
    "    n_classes = test_gen.num_classes #len(test_gen.class_indices)\n",
    "    \n",
    "    # Get true labels and binarize\n",
    "    true_labels = test_gen.classes\n",
    "    true_labels_binarized = label_binarize(true_labels, classes=np.arange(n_classes))\n",
    "    \n",
    "    # Get predicted probabilities\n",
    "    predicted_probs = model.predict(test_gen)\n",
    "\n",
    "    if n_classes == 2:\n",
    "        true_labels_binarized = label_binarize(true_labels, classes=[0, 1])\n",
    "        fpr, tpr, _ = roc_curve(true_labels_binarized[:, 0], predicted_probs[:, 0])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        # Plot ROC curve for binary classification\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.plot(fpr, tpr, lw=2, label='ROC curve (area = {0:0.2f})'.format(roc_auc))\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'{model_name} Receiver Operating Characteristic (ROC) ({data_cfg[\"group\"]} classification task)')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig(f'{model_directory}/{model_name}_{data_cfg[\"group\"]}_ROC.png', dpi=800)\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        true_labels_binarized = label_binarize(true_labels, classes=np.arange(n_classes))\n",
    "\n",
    "        # Compute ROC curve and ROC area for each class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(true_labels_binarized[:, i], predicted_probs[:, i])\n",
    "\n",
    "        # Compute macro-average ROC curve and ROC area\n",
    "        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "        mean_tpr = np.zeros_like(all_fpr)\n",
    "        for i in range(n_classes):\n",
    "            mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "        mean_tpr /= n_classes\n",
    "\n",
    "        fpr[\"macro\"] = all_fpr\n",
    "        tpr[\"macro\"] = mean_tpr\n",
    "        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "        # Plot macro-average ROC curve\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.plot(fpr[\"macro\"], tpr[\"macro\"], lw=2, label='Macro-average ROC curve (area = {0:0.2f})'.format(roc_auc[\"macro\"]))\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'{model_name} Receiver Operating Characteristic (ROC) ({data_cfg[\"group\"]} classification task)')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig(f'{model_directory}/{model_name}_{data_cfg[\"group\"]}_ROC.png', dpi=800)\n",
    "        plt.show()\n",
    "    \n",
    "def plot_confusion_matrix(model_name, predicted_classes, test_gen, data_cfg, model_directory):\n",
    "    conf_matrix = confusion_matrix(test_gen.classes, predicted_classes)\n",
    "    \n",
    "    plt.rcParams['font.size'] = 7\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    class_names = list(test_gen.class_indices.keys())\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'{model_name} Confusion Matrix ({data_cfg[\"group\"]} classification task)')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(f'{model_directory}/{model_name}_{data_cfg[\"group\"]}_CM.png', dpi=800)\n",
    "    plt.show()    \n",
    "\n",
    "def print_classification_report(model_name, predicted_classes, test_gen, data_cfg, model_directory):\n",
    "    #F1-score, precision, and recall\n",
    "    classif_report = classification_report(test_gen.classes, predicted_classes, target_names=test_gen.class_indices.keys(), output_dict=True)\n",
    "    classif_report_df = pd.DataFrame(classif_report).transpose()\n",
    "  \n",
    "    print(\"Classification Report: \\n\")\n",
    "    print(classif_report_df)\n",
    "\n",
    "    with open(f'{model_directory}/{model_name}_{data_cfg[\"group\"]}_classification_report.json', 'w') as f:\n",
    "        json.dump(classif_report, f)\n",
    "\n",
    "def profile_model_peformance(model_name, model, data_cfg, model_directory):\n",
    "    if (data_cfg['self-supervised']):\n",
    "        metric='custom_mse'\n",
    "    else:\n",
    "        metric='accuracy'\n",
    "        \n",
    "    with open(f'{model_directory}/{model_name}_{data_cfg[\"group\"]}_training_history.txt', 'w') as file:\n",
    "        file.write(f'Epoch\\tLoss\\t{metric}\\n')\n",
    "        for epoch, (loss, acc) in enumerate(zip(history.history['loss'], history.history[f'{metric}'])):\n",
    "            file.write(f'{epoch}\\t{loss}\\t{acc}\\n')\n",
    "            \n",
    "    @tf.function\n",
    "    def run_model(inputs):\n",
    "        return model(inputs, training=False)\n",
    "    \n",
    "    # Create a TensorSpec\n",
    "    input_spec = tf.TensorSpec(shape=[experiments[model_name]['batch_size'], experiments[model_name]['image_size'], \n",
    "                                      experiments[model_name]['image_size'], experiments[model_name]['num_channels']], dtype=tf.float32)\n",
    "    \n",
    "   # Start profiling\n",
    "    options = tf.profiler.experimental.ProfilerOptions(host_tracer_level = 3,\n",
    "                                                       python_tracer_level = 1,\n",
    "                                                       device_tracer_level = 1)\n",
    "\n",
    "    tf.profiler.experimental.start(f'{model_directory}/profiler_eval'.format(model_name))\n",
    "    \n",
    "    # Run the model\n",
    "    concrete_func = run_model.get_concrete_function(input_spec)\n",
    "    \n",
    "    # Stop profiling\n",
    "    tf.profiler.experimental.stop()\n",
    "      \n",
    "    frozen_func = convert_variables_to_constants_v2(concrete_func)    \n",
    "    flops = profile(frozen_func.graph, options=ProfileOptionBuilder.float_operation())\n",
    "\n",
    "    with open(f'{model_directory}/{model_name}_{data_cfg[\"group\"]}_GLOPs_Inference.txt', 'w') as f:\n",
    "        f.write('GFLOPs: {}\\n'.format(flops.total_float_ops / 1e9))\n",
    "        f.write('Inference time: {}\\n'.format(inference_time))\n",
    "        \n",
    "def display_evaluation_metrics(model_name, model, history, predicted_classes, inference_time, test_gen, data_cfg, model_directory):\n",
    "    plot_trainval_graph(model_name, history, data_cfg, model_directory)\n",
    "    profile_model_peformance(model_name, model, data_cfg, model_directory)\n",
    "\n",
    "    if (not data_cfg['self-supervised']): # applicable only for downstream tasks\n",
    "        plot_roc_graph(model, model_name, test_gen, data_cfg, model_directory)\n",
    "        plot_confusion_matrix(model_name, predicted_classes, test_gen, data_cfg, model_directory)\n",
    "        print_classification_report(model_name, predicted_classes, test_gen, data_cfg, model_directory)\n",
    "    \n",
    "def run_experiment(model, data_cfg, model_directory):\n",
    "    train_gen, val_gen, trainval_gen, test_gen = get_generators(experiments[model], data_cfg)\n",
    "    class_weights = get_class_weights(val_gen, data_cfg['class_balancing']) \n",
    "    \n",
    "    # Modify 'group' and update model directory to avoid changes to control flow\n",
    "    if (data_cfg['self-supervised']):\n",
    "        data_cfg['group'] = 'encodings'\n",
    "        model_directory = f'models/{current_experiment}/{data_cfg[\"group\"]}'\n",
    "        class_weights = None # discard class weights\n",
    "\n",
    "    os.makedirs(model_directory, exist_ok=True)\n",
    "    best_model, best_hps, history = train_model(model, train_gen, val_gen, trainval_gen, class_weights, data_cfg, model_directory)\n",
    "    if (data_cfg['self-supervised']): #not applicable at pre-training stage\n",
    "        predictions = None\n",
    "        inference_time = None\n",
    "    else:\n",
    "        predictions, inference_time = evaluate_model(best_model, model, test_gen)\n",
    "        \n",
    "    return model, best_model, best_hps, predictions, inference_time, history, test_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15480c41-99b1-4109-9e2d-1df5323431f7",
   "metadata": {},
   "source": [
    "## Experiment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb90241-91d2-4f2f-87c3-7f9abac4db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_experiment(current_experiment, task, tiny, mixup, gauss, reweigh, skip_validation, self_supervised):\n",
    "    experiments = {}\n",
    "    with open(f'models/{current_experiment}-params.json', 'r') as f:\n",
    "        experiments[current_experiment] = json.load(f)\n",
    "    \n",
    "    data_cfg = {\n",
    "        'color_mode': 'rgb',  # options: rgb, grayscale\n",
    "        'validation_split': 1.0,\n",
    "        'malnet_tiny': tiny, # MalNet-tiny speeds up testing\n",
    "        'use_mixup': mixup, # causes overfitting, maybe doesn't make sense at all in context of malware detection\n",
    "        'use_gauss_noise': gauss,\n",
    "        'class_balancing': reweigh,\n",
    "        'skip_validation': skip_validation,\n",
    "        'self-supervised': self_supervised,\n",
    "        'group': task,  # options: binary, type, family\n",
    "        'data_dir': os.getcwd() + '/data/',  # symbolic link directory\n",
    "        'image_dir': '/malnet-images/'  # path where data is located\n",
    "    }\n",
    "    \n",
    "    # # Dataset construction block\n",
    "    if os.path.exists('data'):\n",
    "        shutil.rmtree('data')\n",
    "    \n",
    "    create_image_symlinks(data_cfg)\n",
    "       \n",
    "    np.random.seed(experiments[current_experiment]['seed'])\n",
    "    tf.random.set_seed(experiments[current_experiment]['seed'])\n",
    "\n",
    "    model_directory = f'models/{current_experiment}/{data_cfg[\"group\"]}'\n",
    "\n",
    "    return experiments, data_cfg, model_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbf0929-7a3b-4d16-a70c-78dce51e483d",
   "metadata": {},
   "source": [
    "## Running the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a528c0-59be-42fa-a389-7cfbabaadd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_experiment = 'ViT-SHERLOCK' #'ViT-V', 'ViT-VS', 'ViT-B_16', 'ViT-SHERLOCK', 'ResNet18', 'MobileNetV2'\n",
    "current_task = 'family'\n",
    "\n",
    "experiments, data_cfg, model_directory = setup_experiment(current_experiment, current_task, False, False, False, True, True, False)\n",
    "\n",
    "model_name, best_model, best_hps, predictions, inference_time, history, test_gen = run_experiment(current_experiment, data_cfg, model_directory)\n",
    "\n",
    "if (data_cfg['self-supervised']):\n",
    "    data_cfg['group'] = 'encodings'\n",
    "    model_directory = f'models/{current_experiment}/{data_cfg[\"group\"]}'\n",
    "    \n",
    "display_evaluation_metrics(model_name, best_model, history, predictions, inference_time, test_gen, data_cfg, model_directory)\n",
    "    \n",
    "# Save the best model\n",
    "best_model.save(f'{model_directory}/{model_name}_{data_cfg[\"group\"]}.h5')\n",
    "\n",
    "k.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
